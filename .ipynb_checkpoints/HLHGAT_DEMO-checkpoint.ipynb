{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e74b889c",
   "metadata": {
    "id": "e74b889c"
   },
   "source": [
    "# HL-HGAT Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f861fc0",
   "metadata": {},
   "source": [
    "#### This demo illustrates how to use the HL-HGAT model to perform sex classification on the ABCD dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69329dbe",
   "metadata": {
    "id": "69329dbe"
   },
   "source": [
    "### load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc8652c",
   "metadata": {
    "id": "bfc8652c"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_scatter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m degree, to_undirected\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Data\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mHodge_Cheb_Conv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mHodge_Dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSpherical_mesh\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\HL-HGAT\\lib\\Hodge_Cheb_Conv.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graclus, max_pool\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Data, Batch\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_scatter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scatter\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_self_loops, dense_to_sparse, degree\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, Optional, Tuple, Union\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch_scatter'"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "import torch\n",
    "import torch_geometric.nn as gnn\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Subset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import degree, to_undirected\n",
    "from torch_geometric.data import Data\n",
    "from lib.Hodge_Cheb_Conv import *\n",
    "from lib.Hodge_Dataset import *\n",
    "from lib.Spherical_mesh import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Di1Ws2sr-mVM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Di1Ws2sr-mVM",
    "outputId": "13adeae7-c047-4363-ff60-6924acb4d56d"
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb55f0",
   "metadata": {
    "id": "71fb55f0"
   },
   "source": [
    "## Introduction: Graphs in PyTorch Geometric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476279e2",
   "metadata": {
    "id": "476279e2"
   },
   "source": [
    "A graph is a data structure consisting of a collection of nodes (or vertices) connected by edges. The connections between nodes are represented by an adjacency matrix. In this matrix, if nodes are connected by an edge, the corresponding matrix element is set to one; if not, it is zero. For instance, if the 0-th and 1-st nodes are connected, the element in the first row and second column of the adjacency matrix will be one. The illustrated example depicts a graph with five nodes and seven edges. In the context of neuroscience, a brain functional network can be modeled as a binary undirected graph, where nodes represent distinct brain regions and edges indicate functional connectivity between these regions.\n",
    "\n",
    "<div align='center'>\n",
    "    <img src=fig/Graph_application.png width=\"1200\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e2c28e",
   "metadata": {
    "id": "f5e2c28e"
   },
   "source": [
    "Each graph in [`Torch Geometric`](https://pytorch-geometric.readthedocs.io/en/latest/index.html) is encapsulated within a [`Data`](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data) object. This object contains all necessary information to represent the graph effectively. The `graph` object comprises several key attributes tailored to applications in neuroscience:\n",
    "\n",
    "1. **Graph Connectivity (`edge_index`)**: This attribute represents the adjacency matrix in a sparse matrix format. It stores the connectivity information as a tuple of source and destination node indices for each edge, offering a memory-efficient way to represent the network structure. This is particularly useful in modeling the complex interconnections between brain regions.\n",
    "\n",
    "2. **Node Features (`x`)**: This refers to the attributes of each node, such as fMRI time-series data associated with each brain region, organized in a matrix with dimensions [number of nodes] x [number of features per node].\n",
    "\n",
    "3. **Labels (`y`)**: Each graph can be labeled with clinically relevant information, such as the diagnostic category or demographic characteristics like the sex of each subject. This is crucial for tasks like graph-based classification in neuroscientific studies.\n",
    "\n",
    "4. **Edge Features (`edge_attr`)**: Similar to node features, edge features may include measurements like functional connectivity strength between brain regions, formatted in a matrix with dimensions [number of edges] x [number of features per edge].\n",
    "\n",
    "You can view a concise summary of these attributes and their dimensions by printing the `Data` object using `print(graph)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46273361",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "    <img src=fig/Graph_example.png width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f9338",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cf6f9338",
    "outputId": "65153815-0bca-44ae-c0c3-5f37a132f978"
   },
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([[0, 0, 0, 1, 1, 2, 3],\n",
    "                           [1, 2, 3, 3, 4, 3, 4]], dtype=torch.long)\n",
    "x = torch.tensor([[-1], [0], [1],[0],[-1]], dtype=torch.float)\n",
    "graph = Data(x=x, edge_index=edge_index, y=1, edge_attr=torch.rand(7,1))\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b006477",
   "metadata": {
    "id": "5b006477"
   },
   "source": [
    "### Build HL-filters for node and edge features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd11f2a",
   "metadata": {
    "id": "dbd11f2a"
   },
   "source": [
    "In our approach, we use a boundary operator to describe how nodes and edges in a graph are connected, similar to an adjacency matrix. This helps us understand the relationships within the graph in a clear and structured way. \n",
    "\n",
    "<!-- $$\n",
    "\\partial_1 = \\begin{bmatrix}\n",
    " -1 & -1 & -1 &  0 &  0 &  0 &  0 \\\\\n",
    "  1 &  0 &  0 & -1 & -1 &  0 &  0 \\\\\n",
    "  0 &  1 &  0 &  0 &  0 & -1 &  0 \\\\\n",
    "  0 &  0 &  1 &  1 &  0 &  1 & -1 \\\\\n",
    "  0 &  0 &  0 &  0 &  1 &  0 &  1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$ -->\n",
    "\n",
    "<div align='center'>\n",
    "    <img src=fig/boundary_operator.png width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284edfa4",
   "metadata": {},
   "source": [
    "We use Hodge-Laplacian operators to build HL node and edge filters. The $k$-th Hodge-Laplacian (HL) operator defined as \n",
    "$$\n",
    "\\boldsymbol{\\mathcal{L}}_k = \\boldsymbol{\\partial}_{k+1} \\boldsymbol{\\partial}_{k+1}^{\\top} + \\boldsymbol{\\partial}_k^{\\top} \\boldsymbol{\\partial}_k.\n",
    "$$\n",
    "When $k=0$,  the $0$-th  HL operator is \n",
    "$$\n",
    "\\boldsymbol{\\mathcal{L}}_0= \\boldsymbol{\\partial}_1 \\boldsymbol{\\partial}_1 ^\\top\n",
    "$$\n",
    "over nodes.  When $k=1$,  the $1$-st HL operator is defined over edges as\n",
    "$$\n",
    "\\boldsymbol{\\mathcal{L}}_1 = \\boldsymbol{\\partial}_{2} \\boldsymbol{\\partial}_{2}^{\\top} + \\boldsymbol{\\partial}_1^{\\top} \\boldsymbol{\\partial}_1  \\ .\n",
    "$$\n",
    "\n",
    "We define the convolution as \n",
    "$$\n",
    "f'(\\cdot)=\\sum_{p=0}^{P-1}\\theta_p T_p(\\boldsymbol{\\mathcal{L}}_k) f(\\cdot).\n",
    "$$\n",
    "\n",
    "where $\\theta_p$ is the $p ^{th}$ expansion coefficient associated with the $p ^{th}$ Laguerre polynomial $T_p$.  $T_p$ can be computed \n",
    "from the recurrence relation of $T_{p+1}(\\lambda_k) = \\frac{(2p+1-\\lambda_k) T_{p}(\\lambda_k)- pT_{p-1}(\\lambda_k)}{p+1} $ with $T_0(\\lambda_k) = 1$ and $T_1(\\lambda_k) = 1 - \\lambda_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac6f7a",
   "metadata": {},
   "source": [
    "### Transform a graph to a heterogeneous graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458309e",
   "metadata": {
    "id": "a458309e"
   },
   "source": [
    "In practice, we implement graph convolution using torch-geometric, storing the 0-th Hodge-Laplacian operators as `edge_index_t` and `edge_weight_t`, and the 1-st Hodge-Laplacian operators as `edge_index_s` and `edge_weight_s` in a heterogeneous graph. To handle this efficiently, we utilize `PairData` from PyTorch Geometric instead of the standard `Data` object. `PairData` is tailored to manage the diverse elements of heterogeneous graphs effectively.\n",
    "\n",
    "Within `PairData`, node features are represented as `x_t` and edge features as `x_s`. This setup allows us to treat nodes and edges as separate entity types, which is essential for the heterogeneous nature of the graph. We store only the upper triangle elements of the adjacency matrix in `edge_index` and convert it into the boundary operator $\\partial_1 $ using the `adj2par1` function, which transforms it into a sparse matrix. This method ensures accurate representation and processing of the complex relationships between different graph components. The following is a function to transform a graph into a heterogeneous graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4KdaD1GkmbsY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4KdaD1GkmbsY",
    "outputId": "d1a3be15-8e4d-4aab-8304-51ec528a9855"
   },
   "outputs": [],
   "source": [
    "def graph2hgraph(data):\n",
    "    '''\n",
    "    transform a graph to a heterogeneous graph\n",
    "    '''\n",
    "    edge_index,edge_attr = to_undirected(data.edge_index, data.edge_attr, reduce='min')\n",
    "    idx = edge_index[0]<edge_index[1]\n",
    "    edge_index,edge_attr = edge_index[:,idx], edge_attr[idx]\n",
    "\n",
    "    par1 = adj2par1(edge_index, data.x.shape[0], edge_index.shape[1]).to_dense()\n",
    "    L0 = torch.matmul(par1, par1.T)\n",
    "    lambda0, _ = torch.linalg.eigh(L0)\n",
    "    maxeig = lambda0.max()\n",
    "    L0 = 2*torch.matmul(par1, par1.T)/maxeig\n",
    "    L1 = 2*torch.matmul(par1.T, par1)/maxeig\n",
    "    x_s = edge_attr.view(-1,1)\n",
    "    x_t = data.x\n",
    "    data = PairData(x_s=x_s, edge_index_s=None, edge_weight_s=None,\n",
    "                      x_t=x_t, edge_index_t=None, edge_weight_t=None,\n",
    "                      y = data.y)\n",
    "    edge_index_t, edge_weight_t = dense_to_sparse(L0)\n",
    "    edge_index_s, edge_weight_s = dense_to_sparse(L1)\n",
    "    data.edge_index_t, data.edge_weight_t = edge_index_t, edge_weight_t\n",
    "    data.edge_index_s, data.edge_weight_s = edge_index_s, edge_weight_s\n",
    "    data.num_node1 = data.x_t.shape[0]\n",
    "    data.num_edge1 = data.x_s.shape[0]\n",
    "    data.num_nodes = data.x_t.shape[0]\n",
    "    data.edge_index=edge_index\n",
    "    return data\n",
    "\n",
    "###\n",
    "# hgraph = graph2hgraph(graph)\n",
    "# hgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699afa1",
   "metadata": {
    "id": "0699afa1"
   },
   "source": [
    "### Build heterogeneous graph with brain fMRI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb1f2b2",
   "metadata": {
    "id": "bdb1f2b2"
   },
   "source": [
    "We store all brain fMRI data in a `.mat` file organized as a cell matrix with dimensions corresponding to the number of samples by four columns. Each row within this matrix represents a sample. The first column contains the fMRI time-series data, where each entry has dimensions equal to the number of ROIs multiplied by the number of time points. The second column, which is optional, holds the structural connectivity data with dimensions of number of ROIs by number of ROIs. If structural connectivity (SC) data is unavailable, this column can be left blank. The third column includes subject information, used for prediction or further analysis. The final column records the subject ID for each sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f8af3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "934f8af3",
    "outputId": "38be971a-91af-4303-af9a-050b3ed93cb5"
   },
   "outputs": [],
   "source": [
    "### Load '.mat' dataset\n",
    "name = 'data/DEMO_DATA.mat'\n",
    "matdata = loadmat(name)\n",
    "matdata = matdata['DEMO_DATA']\n",
    "num_samples = len(matdata)\n",
    "num_rois = matdata[0][0].shape[0]\n",
    "print('Number of samples: {} -- Number of ROIs: {}'.\n",
    "      format(num_samples, num_rois))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1dacdc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e1dacdc",
    "outputId": "f9222619-9c8d-4c47-f808-0c250a0c2167"
   },
   "outputs": [],
   "source": [
    "### Compute group-level FC mask\n",
    "k_ratio = 0.25   ## percentage of non-zero values in the mask\n",
    "FC = torch.zeros(num_samples,num_rois,num_rois)\n",
    "for i in range(num_samples):\n",
    "    FC[i] = torch.corrcoef(torch.tensor(matdata[i][0]))\n",
    "mask = FC2mask(FC, k_ratio=k_ratio)\n",
    "masked_FC = mask * FC.mean(dim=0)\n",
    "skeleton = masked_FC.to_sparse()\n",
    "print('Number of functional connectivity after thresholding: {}'.\n",
    "      format(len(skeleton.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312de014",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "312de014",
    "outputId": "3b0c5d9b-c2bd-4283-f688-7a2151b25cd6"
   },
   "outputs": [],
   "source": [
    "par1 = adj2par1(skeleton.indices(), num_rois,\n",
    "         skeleton.indices().shape[-1]).to_dense()\n",
    "L0 = torch.matmul(par1, par1.T)\n",
    "lambda0, _ = torch.linalg.eigh(L0)\n",
    "maxeig = lambda0.max()\n",
    "L0 = 2*torch.matmul(par1, par1.T)/maxeig\n",
    "L1 = 2*torch.matmul(par1.T, par1)/maxeig\n",
    "eit, ewt = dense_to_sparse(L0)\n",
    "eis, ews = dense_to_sparse(L1)\n",
    "graph = PairData(edge_weight_s=ews,edge_index_s=eis,\n",
    "          x_s=skeleton.values().view(-1,1),x_t=torch.ones(num_rois,1),\n",
    "          edge_index_t=eit,edge_weight_t=ewt,\n",
    "          edge_index=skeleton.indices())\n",
    "graph.num_node1 = num_rois\n",
    "graph.num_edge1 = skeleton.indices().shape[1]\n",
    "graph.num_nodes = num_rois\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6b9cf8",
   "metadata": {
    "id": "4b6b9cf8"
   },
   "source": [
    "## Simplex downsampling (graph coarsening)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7b58c1",
   "metadata": {
    "id": "da7b58c1"
   },
   "source": [
    "We store node and edge cluster information in `pos_t` and `pos_s`, respectively. Nodes or edges within the same cluster are assigned the same index in these arrays. Any nodes or edges labeled as `inf` are discarded during the pooling process. Each coarsened heterogeneous graph is then appended to a list, resulting in each sample being represented by a list that contains graphs at various levels of spatial resolution.\n",
    "\n",
    "<div align='center'>\n",
    "    <img src=fig/pooling.png width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba02549",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ba02549",
    "outputId": "ca2b2e5c-3046-458d-8827-097af4193da2"
   },
   "outputs": [],
   "source": [
    "### precompute pooling\n",
    "graphs = [graph]\n",
    "pool_num = 2  ### number of pooling layers\n",
    "for i in range(pool_num):\n",
    "    graph, c_node, c_edge = MLGC_Weight(graphs[i])\n",
    "    graphs[i].pos_s, graphs[i].pos_t = c_edge, c_node\n",
    "    graphs.append(graph)\n",
    "\n",
    "num_nodepedges = []\n",
    "for graph in graphs:\n",
    "    par = adj2par1(graph.edge_index, graph.num_node1, graph.edge_index.shape[-1]).to_dense()\n",
    "    num_nodepedges.append(par.shape[1] + par.shape[0]) ### number of nodes and edges after pooling\n",
    "graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5972630",
   "metadata": {
    "id": "b5972630"
   },
   "source": [
    "## Dataset and graph batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45cef98",
   "metadata": {
    "id": "e45cef98"
   },
   "source": [
    "Neural networks are typically trained in batches. However, when dealing with graphs, the number of nodes or edges can differ between samples. PyTorch Geometric addresses this challenge by constructing a large composite graph for each batch. If we consider `n` samples, each with its own adjacency matrix $\\{A_1, \\dots, A_n\\}$ and node features $\\{X_1, \\dots, X_n\\}$, the adjacency matrix and node features for the batched graph are structured as follows:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "A_1 & 0 & \\cdots & 0 \\\\\n",
    "0 & A_2 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & A_n\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "X = \\begin{bmatrix}\n",
    "X_1 \\\\\n",
    "X_2 \\\\\n",
    "\\vdots \\\\\n",
    "X_n\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "This arrangement ensures that each sample's graph structure is maintained within the batch, allowing for efficient processing of variable-sized graphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f6c74d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62f6c74d",
    "outputId": "b55d4010-1a0c-4912-f036-baf52fc624b1"
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "### mode: if 0, use functional connectivity as edge feature\n",
    "###       if 1, use structural connectivity as edge feature\n",
    "### y_idx: specifies the target element in the subject information to be predicted.\n",
    "###        for example, in the DEMO data, the 9-th element corresponds to sex.\n",
    "dataset = Brain_MLGC_ALL('Brain', matdata, skeleton.indices(), graphs, pool_num=pool_num,\n",
    "                         mode=1, y_idx=8)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "graph_batch = next(iter(train_loader))\n",
    "print(graph_batch[0])\n",
    "# print(graph_batch[-1].ptr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a7e81",
   "metadata": {
    "id": "5e0a7e81"
   },
   "source": [
    "## Implement HL-HGAT\n",
    "\n",
    "<div align='center'>\n",
    "    <img src=fig/HGAT.png width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0895d1",
   "metadata": {
    "id": "ef0895d1"
   },
   "source": [
    "### Temporal convolution of fMRI time-series\n",
    "Hyperparameters of the inception module:\n",
    "- in_channels: number of channels in the inception module\n",
    "- maxpool: size of the sliding window\n",
    "- if_readout: if reduce the time dimension with averaging\n",
    "\n",
    "Inputs of the inception module:\n",
    "- node signal (dimension: number of nodes $\\times$ number of time points)\n",
    "\n",
    "Outputs of the inception module:\n",
    "- node signal (dimension: number of nodes $\\times$ 4*num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a571fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3a571fc",
    "outputId": "08917f1f-262e-4ed5-8fe4-aecabe49f186"
   },
   "outputs": [],
   "source": [
    "node_dim = 32\n",
    "time_pool_step = 5\n",
    "timeconv = Inception1D(in_channels=node_dim,\n",
    "                       maxpool=time_pool_step, if_readout=True)\n",
    "print('size of the input node signal: {}'.format(graph_batch[0].x_t.shape))\n",
    "x_t = timeconv(graph_batch[0].x_t)\n",
    "print('size of the output node signal: {}'.format(x_t.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034db672",
   "metadata": {
    "id": "034db672"
   },
   "source": [
    "### HL-filters\n",
    "Convolutional layer on node and edge signals\n",
    "\n",
    "Hyperparameters of the inception module:\n",
    "- layers: number of HL-filtering layer\n",
    "- channels: number of filters in each layer\n",
    "- K: polynomial order\n",
    "- node_dim: input node dimension\n",
    "- edge_dim: input edge dimension\n",
    "\n",
    "Inputs of the inception module:\n",
    "- x_t, edge_index_t, edge_weight_t, x_s, edge_index_s, edge_weight_s\n",
    "\n",
    "Outputs of the inception module:\n",
    "- node and edge signal (dimension: number of nodes/edges $\\times$ filters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189741b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "189741b6",
    "outputId": "e8b2cb3b-1631-4a60-c6a0-ca6efe123faf"
   },
   "outputs": [],
   "source": [
    "edge_dim = 1\n",
    "dropout_ratio = 0.25\n",
    "leaky_slope = 0.1\n",
    "conv =  HL_filter(layers=1, channels=32, K=4, node_dim=node_dim*2,\n",
    "                  edge_dim=edge_dim, dropout_ratio=dropout_ratio,\n",
    "                  leaky_slope=leaky_slope,if_dense=False)\n",
    "print('size of the input node and edge signal: {}, {}'.format(x_t.shape, graph_batch[0].x_s.shape))\n",
    "x_t, x_s = conv(x_t, graph_batch[0].edge_index_t, graph_batch[0].edge_weight_t,\n",
    "                graph_batch[0].x_s, graph_batch[0].edge_index_s, graph_batch[0].edge_weight_s)\n",
    "print('size of the output node and edge signal: {}, {}'.format(x_t.shape, x_s.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea63316",
   "metadata": {
    "id": "5ea63316"
   },
   "source": [
    "### Multi-simplicial Interaction (MSI)\n",
    "MSI layer on node and edge signals\n",
    "\n",
    "Hyperparameters of MSI:\n",
    "- d: input feature dim\n",
    "- dk: feature dim of key & query\n",
    "- dv: feature dim of value\n",
    "- dl: feature dim of latent\n",
    "- only_att: if true, only output the attention value\n",
    "- sigma: activation function\n",
    "\n",
    "Inputs of MSI:\n",
    "- x_t, x_s: node and edge signals\n",
    "- par_1: boundary operator (sparse matrix, dim: [number of nodes, number of edges])\n",
    "- D: degree matrix (dim: number of nodes)\n",
    "\n",
    "Outputs of MSI:\n",
    "- node and edge signal (dimension: number of nodes/edges $\\times$ filters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119d0227",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "119d0227",
    "outputId": "7bc08e13-0cb4-4922-cf52-2485b87d6dc0"
   },
   "outputs": [],
   "source": [
    "msi = MSI(d=32, dv=32)\n",
    "par_1 = adj2par1(graph_batch[0].edge_index, x_t.shape[0], x_s.shape[0])\n",
    "D = degree(graph_batch[0].edge_index.view(-1),num_nodes=x_t.shape[0]) + 1e-6\n",
    "print('size of the input node and edge signal: {}, {}'.format(x_t.shape, x_s.shape))\n",
    "x_t, x_s = msi(x_t, x_s, par_1, D)\n",
    "print('size of the output node and edge signal: {}, {}'.format(x_t.shape, x_s.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62491662",
   "metadata": {
    "id": "62491662"
   },
   "source": [
    "### Simplicial Attention Pooling (SAP)\n",
    "\n",
    "<div align='center'>\n",
    "    <img src=fig/pooling_architecture.png width=\"800\">\n",
    "</div>\n",
    "\n",
    "Hyperparameters of SAP:\n",
    "- d: input feature dim\n",
    "- dk: feature dim of key & query\n",
    "\n",
    "Inputs of MSI:\n",
    "- x_t, x_s: node and edge signals\n",
    "- par_1: boundary operator (sparse matrix, dim: [number of nodes, number of edges])\n",
    "- D: degree matrix (dim: number of nodes)\n",
    "- graph_batch: a list of graphs\n",
    "- pos_ts, pos_ss: two lists of the node and edge cluster indices\n",
    "- k: spatial level, increase 1 after pooling\n",
    "\n",
    "Outputs of MSI:\n",
    "- x_t, x_s: node and edge signals\n",
    "- par_1: boundary operator (sparse matrix, dim: [number of nodes, number of edges])\n",
    "- D: degree matrix (dim: number of nodes)\n",
    "- k: spatial level\n",
    "- edge_index_t ... : updated Hodge-Laplacian operator\n",
    "- att_t, att_s: node and edge attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf2da1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaaf2da1",
    "outputId": "6b7bdb23-467d-4575-ef8f-df8f351e83e2"
   },
   "outputs": [],
   "source": [
    "fc = SAPool(d=32, dk=16)\n",
    "pos_ts, pos_ss = [], []\n",
    "for p in range(pool_num):\n",
    "    n_batch = torch.cat( [torch.tensor([i]*nn) for i,nn in enumerate(graph_batch[p].num_node1)], dim=-1)\n",
    "    s_batch = torch.cat( [torch.tensor([i]*nn) for i,nn in enumerate(graph_batch[p].num_edge1)], dim=-1)\n",
    "    n_ahead = torch.cumsum(torch.cat([torch.zeros(1),graph_batch[p+1].num_node1],dim=-1), dim=0,\n",
    "                           dtype=torch.long)[:-1]\n",
    "    s_ahead = torch.cumsum(torch.cat([torch.zeros(1),graph_batch[p+1].num_edge1],dim=-1), dim=0,\n",
    "                           dtype=torch.long)[:-1]\n",
    "    pos_ts.append((graph_batch[p].pos_t.view(-1) + n_ahead[n_batch]).view(-1,1))\n",
    "    pos_ss.append((graph_batch[p].pos_s.view(-1) + s_ahead[s_batch]).view(-1,1))\n",
    "\n",
    "k = 0  ## spatial level, increase 1 after pooling\n",
    "print('size of the input node and edge signal: {}, {}'.format(x_t.shape, x_s.shape))\n",
    "x_t, x_s, par_1, D, k, edge_index_t, edge_weight_t, edge_index_s, edge_weight_s, att_t, att_s = fc(x_t, x_s, par_1, D, graph_batch, pos_ts, pos_ss, k, device='cpu')\n",
    "print('size of the output node and edge signal: {}, {}'.format(x_t.shape, x_s.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7eafdb",
   "metadata": {
    "id": "eb7eafdb"
   },
   "source": [
    "### HL-HGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7473f710",
   "metadata": {
    "id": "7473f710"
   },
   "outputs": [],
   "source": [
    "class HL_HGAT(torch.nn.Module):\n",
    "    def __init__(self, num_layers=[2,2,2], channels=[32,64,128], mlp_channels=[],\n",
    "                 K=4, node_dim=64, time_pool_step=5, edge_dim=1, keig=0, dk=64,\n",
    "                 num_classes=1, dropout_ratio=0.0, pool_num=2, leaky_slope = 0.1,\n",
    "                 num_nodepedge=None):\n",
    "        super(HL_HGAT, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.channels = channels\n",
    "        self.mlp_channels = mlp_channels\n",
    "        self.node_dim = node_dim + keig\n",
    "        self.edge_dim = edge_dim + keig\n",
    "        self.initial_channel = self.channels[0]\n",
    "        self.pool_loc = [i for i in range(pool_num)]\n",
    "        self.keig = keig # number of eigenvalue\n",
    "        self.num_nodepedge = num_nodepedge\n",
    "\n",
    "        ## Temporal convolution of fMRI time-series\n",
    "        self.node_embedding = Inception1D(in_channels=node_dim, maxpool=time_pool_step,\n",
    "                                          if_readout=True)\n",
    "        ## Initial HL-filter\n",
    "        self.HL_init_conv = HL_filter(layers=1, channels=self.initial_channel, K=K,\n",
    "                                      node_dim=self.node_dim*2, edge_dim=self.edge_dim,\n",
    "                                      dropout_ratio=dropout_ratio, leaky_slope=leaky_slope,\n",
    "                                      if_dense=False)\n",
    "        gcn_insize = self.initial_channel\n",
    "\n",
    "        ## multiple blocks\n",
    "        for i, gcn_outsize in enumerate(self.channels):\n",
    "            if self.num_layers[i] == 0:\n",
    "                continue\n",
    "            fc = HL_filter(layers=self.num_layers[i], channels=gcn_outsize, K=K, node_dim=gcn_insize,\n",
    "                           edge_dim=gcn_insize, dropout_ratio=dropout_ratio, leaky_slope=leaky_slope)\n",
    "            setattr(self, 'HLconv{}'.format(i), fc)\n",
    "            gcn_insize = gcn_insize + self.num_layers[i]*gcn_outsize\n",
    "\n",
    "            fc = MSI(d=gcn_insize, dv = gcn_outsize)\n",
    "            setattr(self, 'MSI{}'.format(i), fc)\n",
    "            gcn_insize = gcn_insize + gcn_outsize\n",
    "\n",
    "            if i in self.pool_loc:\n",
    "                fc = SAPool(d=gcn_insize, dk=dk)\n",
    "                setattr(self, 'SAP{}'.format(i), fc)\n",
    "\n",
    "        ## Readout\n",
    "        self.t_readout = Linear(gcn_outsize, 1)\n",
    "        self.s_readout = Linear(gcn_outsize, 1)\n",
    "\n",
    "        ## output layer\n",
    "        mlp_insize = self.num_nodepedge\n",
    "        for i, mlp_outsize in enumerate(mlp_channels):\n",
    "            fc = nn.Sequential(\n",
    "                Linear(mlp_insize, mlp_outsize),\n",
    "                nn.BatchNorm1d(mlp_outsize),\n",
    "                nn.LeakyReLU(negative_slope=leaky_slope),\n",
    "                nn.Dropout(dropout_ratio),\n",
    "                )\n",
    "            setattr(self, 'mlp%d' % i, fc)\n",
    "            mlp_insize = mlp_outsize\n",
    "\n",
    "        self.out = Linear(mlp_insize, num_classes)\n",
    "\n",
    "    def forward(self, datas, device='cuda:0'):\n",
    "        data = datas[0].to(device)\n",
    "        # 1. node & edge postion\n",
    "        pos_ts, pos_ss = [], []\n",
    "        for p in range(len(self.pool_loc)):\n",
    "            n_batch = torch.cat( [torch.tensor([i]*nn) for i,nn in enumerate(datas[p].num_node1)], dim=-1)\n",
    "            n_batch = n_batch.to(device)\n",
    "            s_batch = torch.cat( [torch.tensor([i]*nn) for i,nn in enumerate(datas[p].num_edge1)], dim=-1)\n",
    "            s_batch = s_batch.to(device)\n",
    "            n_ahead = torch.cumsum(torch.cat([torch.zeros(1),datas[p+1].num_node1],dim=-1).to(device), dim=0, dtype=torch.long)[:-1]\n",
    "            s_ahead = torch.cumsum(torch.cat([torch.zeros(1),datas[p+1].num_edge1],dim=-1).to(device), dim=0, dtype=torch.long)[:-1]\n",
    "            pos_ts.append((datas[p].pos_t.to(device).view(-1) + n_ahead[n_batch]).view(-1,1))\n",
    "            pos_ss.append((datas[p].pos_s.to(device).view(-1) + s_ahead[s_batch]).view(-1,1))\n",
    "\n",
    "        x_s, edge_index_s, edge_weight_s = data.x_s, data.edge_index_s, data.edge_weight_s\n",
    "        x_t, edge_index_t, edge_weight_t = data.x_t, data.edge_index_t, data.edge_weight_t\n",
    "\n",
    "        # 2. Obtain node & edge embeddings\n",
    "        x_t = self.node_embedding(x_t)\n",
    "\n",
    "        x_t0, x_s0 = self.HL_init_conv(x_t, edge_index_t, edge_weight_t, x_s, edge_index_s, edge_weight_s)\n",
    "        k = 0\n",
    "        par_1 = adj2par1(datas[k].edge_index.to(device), x_t0.shape[0], x_s0.shape[0])\n",
    "        D = degree(datas[k].edge_index.view(-1).to(device),num_nodes=x_t0.shape[0]) + 1e-6\n",
    "        for i, channel in enumerate(self.num_layers):\n",
    "            if channel == 0:\n",
    "                continue\n",
    "            fc = getattr(self, 'HLconv{}'.format(i))\n",
    "            x_t0, x_s0 = fc(x_t0, edge_index_t, edge_weight_t, x_s0, edge_index_s, edge_weight_s,par_1,D)\n",
    "            fc = getattr(self, 'MSI{}'.format(i))\n",
    "            x_t, x_s = fc(x_t0, x_s0, par_1, D)\n",
    "            x_t0 = torch.cat([x_t0, x_t], dim=-1)\n",
    "            x_s0 = torch.cat([x_s0, x_s], dim=-1)\n",
    "\n",
    "            # structural pooling\n",
    "            if i in self.pool_loc:\n",
    "                fc = getattr(self, 'SAP{}'.format(i))\n",
    "                x_t0, x_s0, par_1, D, k, edge_index_t, edge_weight_t, edge_index_s, edge_weight_s, att_t, att_s = fc(x_t0, x_s0, par_1, D,\n",
    "                                                                                                                     datas, pos_ts, pos_ss,\n",
    "                                                                                                                     k, device=device)\n",
    "                if k == 1:\n",
    "                    node_att, edge_att = att_t.view(data.num_graphs,-1), att_s.view(data.num_graphs,-1)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x_t = self.t_readout(x_t)\n",
    "        x_s = self.s_readout(x_s)\n",
    "        x = torch.cat([x_s.view(data.num_graphs,-1), x_t.view(data.num_graphs,-1)], dim=-1)\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        for i, _ in enumerate(self.mlp_channels):\n",
    "            fc = getattr(self, 'mlp%d' % i)\n",
    "            x = fc(x)\n",
    "\n",
    "        return self.out(x), x, node_att, edge_att"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb77b20",
   "metadata": {
    "id": "0bb77b20"
   },
   "source": [
    "### Building HL-HGAT with Three Blocks\n",
    "\n",
    "HL-HGAT is designed to be highly modular, allowing users to construct a model with multiple blocks. Each block consists of several HL-filtering layers and a Multi-Simplicial Interaction (MSI) layer. The configuration of these blocks is controlled by the `num_layers` and `channels` lists, which determine the number of HL-filtering layers and the number of channels for each layer, respectively.\n",
    "\n",
    "##### Hyperparameters of the HL-HGAT:\n",
    "- **num_layers**: List detailing the number of HL-filtering layers in each block (e.g., if there are two blocks, the length of this list is 2).\n",
    "- **num_channels**: List specifying the number of channels in each HL-filtering layer, with the same length as `num_layers`.\n",
    "- **mlp_channels**: List indicating the number of neurons in each fully connected layer (empty if there are no hidden layers).\n",
    "- **init_time_conv**: Specifies the number of channels in the initial temporal convolutional layer.\n",
    "- **node_dim**: Defines the number of channels in the inception module.\n",
    "- **time_pool_step**: Size of the sliding window used in pooling operations.\n",
    "\n",
    "##### Inputs of the Inception Module:\n",
    "- **graph_batch**: A list of batched graphs prepared for processing.\n",
    "\n",
    "##### Outputs of the Inception Module:\n",
    "- **pred**: The prediction results.\n",
    "- **latent**: The latent representation from the final layer.\n",
    "- **node_att**, **edge_att**: Node and edge attention metrics derived from the first layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99b8eee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f99b8eee",
    "outputId": "cebef9dd-1e6d-4ceb-8111-93caf3395004"
   },
   "outputs": [],
   "source": [
    "pool_num = 2\n",
    "#### An example of a HL-HGAT with three blocks and one fully connected layer\n",
    "model = HL_HGAT(num_layers=[2,2,2], channels=[32,64,128], mlp_channels=[256],\n",
    "                K=4, node_dim=64, time_pool_step=5, edge_dim=1,\n",
    "                num_classes=1, dropout_ratio=0.0, pool_num=pool_num, leaky_slope = 0.1,\n",
    "                keig=0, dk=64, num_nodepedge=num_nodepedges[pool_num]).to(device)\n",
    "pred, latent, node_att, edge_att = model(graph_batch, device=device)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f466c9e9",
   "metadata": {
    "id": "f466c9e9"
   },
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5944a0af",
   "metadata": {
    "id": "5944a0af"
   },
   "outputs": [],
   "source": [
    "#### define training and testing sets\n",
    "batch_size = 5\n",
    "trainset = Subset(dataset, range(10))\n",
    "testset = Subset(dataset, range(20,30))\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size)\n",
    "\n",
    "#### define loss function\n",
    "lr, l2 = 1e-4, 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2)\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion1 = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f02c8da",
   "metadata": {
    "id": "9f02c8da"
   },
   "outputs": [],
   "source": [
    "def train(loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    y_pred, y = [], []\n",
    "    for data in loader:  # Iterate in batches over the training dataset.\n",
    "        out,_,_,_ = model(data, device=device)\n",
    "        loss = criterion( out, data[0].y.view(-1,1))  # Compute the loss.\n",
    "        total_loss += loss*data[0].num_graphs\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        y_pred.extend(out.squeeze())\n",
    "        y.extend(data[0].y.squeeze())\n",
    "\n",
    "    y_pred, y = torch.tensor(y_pred), torch.tensor(y)\n",
    "    y_pred = (y_pred - y_pred.mean()) / y_pred.std()\n",
    "    y = (y - y.mean()) / y.std()\n",
    "    return torch.sqrt(total_loss/len(loader.dataset)), torch.mean(y_pred*y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de04566",
   "metadata": {
    "id": "3de04566"
   },
   "outputs": [],
   "source": [
    "def test(loader):\n",
    "    model.eval()\n",
    "    y_pred, y = [], []\n",
    "    total_loss, rmse = 0, 0\n",
    "\n",
    "    for idx, data in enumerate(loader):  # Iterate in batches over the test dataset.\n",
    "        with torch.no_grad():\n",
    "            out,_,_,_ = model(data, device=device)\n",
    "        loss = criterion1(out, data[0].y.view(-1,1))   # Compute the loss.\n",
    "        rmse += criterion(out, data[0].y.view(-1,1))*data[0].num_graphs\n",
    "        total_loss += loss * data[0].num_graphs\n",
    "        y_pred.extend(out.squeeze())\n",
    "        y.extend(data[0].y.squeeze())\n",
    "\n",
    "    y_pred, y = torch.tensor(y_pred), torch.tensor(y)\n",
    "    y_pred = (y_pred - y_pred.mean()) / y_pred.std()\n",
    "    y = (y - y.mean()) / y.std()\n",
    "    return torch.mean(y_pred*y), total_loss/len(loader.dataset), torch.sqrt(rmse/len(loader.dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa79df",
   "metadata": {
    "id": "bdfa79df"
   },
   "outputs": [],
   "source": [
    "# num_epoch = 4  ### maximum number of epochs\n",
    "# import time\n",
    "# start = time.time()\n",
    "# for epoch in range(0, num_epoch):\n",
    "#     total_loss, train_corr = train(train_loader)\n",
    "#     elapsed = (time.time()-start) / 60\n",
    "#     print(f'Epoch: {epoch:03d}, time: {elapsed:.2f} Train Loss: {total_loss:.4f}, Train Corr: {train_corr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b579b77",
   "metadata": {
    "id": "3b579b77"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edfae94",
   "metadata": {
    "id": "5edfae94"
   },
   "source": [
    "Given a pretrained model and part of testing data, we can visualize the results in a couple of ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J1CmWL96IlD2",
   "metadata": {
    "id": "J1CmWL96IlD2"
   },
   "outputs": [],
   "source": [
    "### Load and preprocess testing data\n",
    "edge_indices = torch.load('data/HGAT_graphs.pth')\n",
    "skeleton = edge_indices[0]\n",
    "num_rois = 268\n",
    "par1 = adj2par1(edge_indices[0], num_rois, edge_indices[0].shape[-1]).to_dense()\n",
    "L0 = torch.matmul(par1, par1.T)\n",
    "lambda0, _ = torch.linalg.eigh(L0)\n",
    "maxeig = lambda0.max()\n",
    "L0 = 2*torch.matmul(par1, par1.T)/maxeig\n",
    "L1 = 2*torch.matmul(par1.T, par1)/maxeig\n",
    "eit, ewt = dense_to_sparse(L0)\n",
    "eis, ews = dense_to_sparse(L1)\n",
    "graph = PairData(x_s=torch.zeros(edge_indices[0].shape[1],1),edge_index_s=eis,\n",
    "                x_t=torch.ones(num_rois,1),edge_index_t=eit,edge_weight_t=ewt,\n",
    "                edge_weight_s=ews,edge_index=edge_indices[0])\n",
    "graph.pos_s, graph.pos_t = edge_indices[2], edge_indices[3]\n",
    "graph.num_node1 = num_rois\n",
    "graph.num_edge1 = edge_indices[0].shape[1]\n",
    "graph.num_nodes = num_rois\n",
    "graphs = [graph]\n",
    "\n",
    "num_rois = int(edge_indices[1].max()+1)\n",
    "par1 = adj2par1(edge_indices[1], num_rois, edge_indices[1].shape[-1]).to_dense()\n",
    "L0 = torch.matmul(par1, par1.T)\n",
    "lambda0, _ = torch.linalg.eigh(L0)\n",
    "maxeig = lambda0.max()\n",
    "L0 = 2*torch.matmul(par1, par1.T)/maxeig\n",
    "L1 = 2*torch.matmul(par1.T, par1)/maxeig\n",
    "eit, ewt = dense_to_sparse(L0)\n",
    "eis, ews = dense_to_sparse(L1)\n",
    "graph = PairData(x_s=torch.zeros(edge_indices[1].shape[1],1),edge_index_s=eis,\n",
    "                x_t=torch.ones(num_rois,1),edge_index_t=eit,edge_weight_t=ewt,\n",
    "                edge_weight_s=ews,edge_index=edge_indices[1])\n",
    "graph.pos_s, graph.pos_t = torch.zeros(edge_indices[1].shape[1],1), torch.ones(num_rois,1)\n",
    "graph.num_node1 = num_rois\n",
    "graph.num_edge1 = edge_indices[1].shape[1]\n",
    "graph.num_nodes = num_rois\n",
    "graphs.append(graph)\n",
    "pool_num = 1\n",
    "num_nodepedge = graphs[pool_num].num_edge1 + graphs[pool_num].num_node1\n",
    "dataset = Brain_MLGC_ALL('Brain', matdata, skeleton, graphs, pool_num=pool_num)\n",
    "dataloader = DataLoader(dataset, batch_size=5, shuffle=False)\n",
    "graph_batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5AY5CI4PTYm0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5AY5CI4PTYm0",
    "outputId": "1a6e9a05-6d38-47bb-b027-0e6f778954ac"
   },
   "outputs": [],
   "source": [
    "### Load pretrained model\n",
    "pretrained_model = HL_HGAT(num_layers=[3,3], channels=[64,128], mlp_channels=[256],\n",
    "                            K=4, node_dim=64, time_pool_step=5, edge_dim=1, num_classes=1,\n",
    "                            dropout_ratio=0.0, pool_num=pool_num, leaky_slope = 0.1,\n",
    "                            keig=0, dk=64, num_nodepedge=num_nodepedge).to(device)\n",
    "pretrained_model.load_state_dict(torch.load('weights/HLHGAT_ABCD_FOLD0.pt',map_location=torch.device(device)))\n",
    "# pred, latent, node_att, edge_att = pretrained_model(graph_batch, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ded070",
   "metadata": {
    "id": "23ded070"
   },
   "source": [
    "We can visualize the latent representation of each subject with different embedding methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ygLOIHrlrMGx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ygLOIHrlrMGx",
    "outputId": "bb46dbf6-bb7c-4812-80f2-a71c71f29625"
   },
   "outputs": [],
   "source": [
    "def visualize(loader, model, device='cuda:0'):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    ys, xs = [], []\n",
    "    node_atts, edge_atts = [], []\n",
    "\n",
    "    for idx, data in enumerate(loader):  # Iterate in batches over the training/test dataset.\n",
    "        ys.append(data[0].y.view(-1,1).cpu())\n",
    "        with torch.no_grad():\n",
    "            y, x, node_att, edge_att = model(data, device=device)\n",
    "            xs.append(x.detach().cpu())\n",
    "            y_pred.append(y.detach().cpu())\n",
    "            node_atts.append(node_att.detach().cpu())\n",
    "            edge_atts.append(edge_att.detach().cpu())\n",
    "    xs = torch.cat(xs, dim=0)\n",
    "    ys = torch.cat(ys, dim=0)\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    node_atts = torch.cat(node_atts, dim=0)\n",
    "    edge_atts = torch.cat(edge_atts, dim=0)\n",
    "    return ys, torch.sigmoid(y_pred), xs, node_atts, edge_atts\n",
    "\n",
    "ys, y_pred, xs, node_atts, edge_atts = visualize(dataloader, pretrained_model, device=device)\n",
    "y_pred1 = y_pred > 0.5\n",
    "y_pred1 = y_pred1.to(torch.int)\n",
    "\n",
    "### accuracy\n",
    "print('Accuracy = {:.4f}'.format(torch.count_nonzero(y_pred1 == ys)/len(ys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b36986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_latent_space(features, labels, method='tsne', perplexity=30, n_iter=1000, random_state=42):\n",
    "    \"\"\"\n",
    "    Visualize the latent representation of the feature map.\n",
    "\n",
    "    Parameters:\n",
    "    - features (torch.Tensor): The feature map with shape [number of samples, number of features].\n",
    "    - labels (torch.Tensor): The label matrix with shape [number of samples].\n",
    "    - method (str): The dimensionality reduction method to use ('tsne' or 'pca').\n",
    "    - perplexity (int): The perplexity parameter for t-SNE.\n",
    "    - n_iter (int): The number of iterations for t-SNE.\n",
    "    - random_state (int): The random seed for reproducibility.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert tensors to numpy arrays\n",
    "    features_np = features.cpu().numpy()\n",
    "    labels_np = labels.cpu().numpy()\n",
    "\n",
    "    if method == 'tsne':\n",
    "        tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=n_iter, random_state=random_state)\n",
    "        reduced_features = tsne.fit_transform(features_np)\n",
    "    elif method == 'pca':\n",
    "        pca = PCA(n_components=2, random_state=random_state)\n",
    "        reduced_features = pca.fit_transform(features_np)\n",
    "    else:\n",
    "        raise ValueError(\"Method should be either 'tsne' or 'pca'\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=labels_np, cmap='viridis', alpha=0.6)\n",
    "    plt.colorbar(scatter, label='Label')\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.title(f'{method.upper()} Visualization of Latent Space')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize using t-SNE\n",
    "visualize_latent_space(xs, ys.view(-1), method='tsne', perplexity=5, n_iter=1000)\n",
    "\n",
    "# Visualize using PCA\n",
    "visualize_latent_space(xs, ys.view(-1), method='pca')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc7d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, normalize=False):\n",
    "    \"\"\"\n",
    "    Plot the confusion matrix for binary classification.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true (array-like): True labels.\n",
    "    - y_pred (array-like): Predicted labels.\n",
    "    - normalize (bool): Whether to normalize the confusion matrix.\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Normalize confusion matrix if required\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "    disp.plot(cmap=plt.cm.Blues, values_format='.2f' if normalize else 'd')\n",
    "    plt.title('Confusion Matrix' + (' (Normalized)' if normalize else ''))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(ys.view(-1), y_pred1.view(-1), normalize=False)\n",
    "\n",
    "# # Plot normalized confusion matrix\n",
    "# plot_confusion_matrix(y_true, y_pred, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4126bd",
   "metadata": {},
   "source": [
    "## Citation\n",
    "If you find this work useful, please cite our IPMI 2023 paper:\n",
    "```bash\n",
    "@inproceedings{huang2023heterogeneous,\n",
    "  title={Heterogeneous Graph Convolutional Neural Network via Hodge-Laplacian for Brain Functional Data},\n",
    "  author={Huang, Jinghan and Chung, Moo K and Qiu, Anqi},\n",
    "  booktitle={International Conference on Information Processing in Medical Imaging},\n",
    "  pages={278--290},\n",
    "  year={2023},\n",
    "  organization={Springer}\n",
    "}\n",
    "```\n",
    "If you are using MSI and SAP modules, please cite our new submission:\n",
    "```\n",
    "@article{huang2024advancing,\n",
    "  title={Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and Attention Mechanism Approach for Heterogeneous Graph-Structured Data},\n",
    "  author={Huang, Jinghan and Chen, Qiufeng and Bian, Yijun and Zhu, Pengli and Chen, Nanguang and Chung, Moo K and Qiu, Anqi},\n",
    "  journal={arXiv preprint arXiv:2403.06687},\n",
    "  year={2024}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
